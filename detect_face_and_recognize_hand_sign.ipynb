{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load face detector\n",
    "PATH_TO_LABELS = './models/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "detection_model_path = './models/face_tf_trt_FP16'\n",
    "\n",
    "\n",
    "# Load hand sign recognizer\n",
    "hand_sign_classes = [\"0_front\", \"1_back\", \"1_front\", \"2_back\", \"2_front\", \"5_front\", \"ILU\"]\n",
    "# classification_model_path = './models/hand_sign_tf_trt_FP16'\n",
    "# classification_model_path = './models/hand_sign_saved_model'\n",
    "classification_model_path = './models/hand_sign.h5'\n",
    "\n",
    "face_detector = load_model(detection_model_path)\n",
    "# hand_sign_recongizer = load_model(classification_model_path)\n",
    "hand_sign_recongizer = tf.keras.models.load_model(classification_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                    for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_absolute(im_height, im_width, box):\n",
    "    box_abs = []\n",
    "    box_abs = [box[0] * im_height,\n",
    "               box[1] * im_width,\n",
    "               box[2] * im_height,\n",
    "               box[3] * im_width]\n",
    "    \n",
    "    return box_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hand_id_dict(face_id_dict):\n",
    "    hand_id_dict = {}\n",
    "    for face_id in face_id_dict.items():\n",
    "        face_box = face_id[0]\n",
    "        \n",
    "        y_min = face_box[0]\n",
    "        x_min = face_box[1]\n",
    "        y_max = face_box[2]\n",
    "        x_max = face_box[3]\n",
    "\n",
    "        hand_box = [y_min, x_min-(x_max-x_min), y_max, x_min]\n",
    "\n",
    "        x_offset = (hand_box[3]-hand_box[1])*0.5\n",
    "        y_offset = (hand_box[2]-hand_box[0])*0.5\n",
    "\n",
    "        hand_box[0] -= 0.5*y_offset\n",
    "        hand_box[1] -= 2.5*x_offset\n",
    "        hand_box[2] += 1.5*y_offset\n",
    "\n",
    "        for i in range(len(hand_box)):\n",
    "            if(hand_box[i] <= 0.0):\n",
    "                hand_box[i] = 0.0\n",
    "            elif(hand_box[i] >= 1.0):\n",
    "                hand_box[i] = 1.0\n",
    "        \n",
    "        hand_id_dict[tuple(hand_box)] = face_id[1]\n",
    "\n",
    "    return hand_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Saved Model style\n",
    "# def predict_hand_sign(image):\n",
    "#     image = cv2.resize(image, (150, 150))\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "#     input_img = np.expand_dims(image, axis=0)    \n",
    "#     input_img = input_img.astype(np.float32) / 255.\n",
    "#     input_tensor = tf.constant(input_img)\n",
    "    \n",
    "#     result = hand_sign_recongizer(input_tensor)\n",
    "#     preds = result['dense_1'].numpy()\n",
    "#     return preds\n",
    "\n",
    "# Keras model(*.h5) style\n",
    "def predict_hand_sign(image):\n",
    "    image = cv2.resize(image, (150, 150))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    input_img = np.expand_dims(image, axis=0)    \n",
    "    input_img = input_img.astype(np.float32) / 255.\n",
    "    \n",
    "    preds = hand_sign_recongizer(input_img)\n",
    "    return preds\n",
    "\n",
    "    \n",
    "def predict_hand_sign_and_visualize(image, hand_id_dict):\n",
    "    im_height, im_width, _ = image.shape\n",
    "    \n",
    "    box_to_display_str_map = collections.defaultdict(list)\n",
    "    box_to_track_ids_map = {}\n",
    "\n",
    "    for hand_id in hand_id_dict.items():\n",
    "        hand_box = convert_to_absolute(im_height, im_width, list(hand_id[0]))\n",
    "        hand_img = image[int(hand_box[0]):int(hand_box[2]),\n",
    "                         int(hand_box[1]):int(hand_box[3])]\n",
    "        \n",
    "        if hand_img.size == 0:\n",
    "            continue\n",
    "            \n",
    "        preds = predict_hand_sign(np.array(hand_img))\n",
    "        \n",
    "        if np.amax(preds[0]) > 0.85:\n",
    "            class_idx = np.argmax(preds[0])\n",
    "            class_name = hand_sign_classes[class_idx]\n",
    "            class_score = int(np.amax(preds[0])*100)\n",
    "            display_str = str(class_name)\n",
    "            display_str = '{}: {}%'.format(display_str, class_score)\n",
    "        else:\n",
    "            display_str = 'Try Again'\n",
    "        \n",
    "        box_to_display_str_map[hand_id[0]].append(display_str)\n",
    "            \n",
    "    for box, display_str in box_to_display_str_map.items():\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        vis_util.draw_bounding_box_on_image_array(image,\n",
    "                                                  ymin,\n",
    "                                                  xmin,\n",
    "                                                  ymax,\n",
    "                                                  xmax,\n",
    "                                                  color='green',\n",
    "                                                  thickness=4,\n",
    "                                                  display_str_list=display_str,\n",
    "                                                  use_normalized_coordinates=True)\n",
    "    \n",
    "#     return box_to_track_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Frame's Width, Height\n",
    "FRAME_WIDTH = 640\n",
    "FRAME_HEIGHT = 480\n",
    "\n",
    "# Initialize webcam feed\n",
    "# video = cv2.VideoCapture(0)\n",
    "video = cv2.VideoCapture('/home/young/Desktop/test/0.mp4')\n",
    "\n",
    "\n",
    "ret = video.set(3, FRAME_WIDTH)\n",
    "ret = video.set(4, FRAME_HEIGHT)\n",
    "\n",
    "person_ids = list(range(10))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = video.read()\n",
    "    output_dict = run_inference_for_single_image(face_detector, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    face_id_dict = vis_util.visualize_boxes_and_labels_on_image_array(image=frame,\n",
    "                                                                      boxes=output_dict['detection_boxes'],\n",
    "                                                                      classes=output_dict['detection_classes'],\n",
    "                                                                      scores=output_dict['detection_scores'],\n",
    "                                                                      category_index=category_index,\n",
    "                                                                      track_ids = person_ids,\n",
    "                                                                      use_normalized_coordinates=True,\n",
    "                                                                      line_thickness=8)\n",
    "    \n",
    "    hand_id_dict = convert_to_hand_id_dict(face_id_dict)\n",
    "    predict_hand_sign_and_visualize(frame, hand_id_dict)\n",
    "    \n",
    "    cv2.imshow('Object detector', frame)\n",
    "    \n",
    "    # Press 'esc' to quit\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
