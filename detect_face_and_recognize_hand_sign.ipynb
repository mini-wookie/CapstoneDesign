{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load face detector\n",
    "detection_model_path = './models/face_tf_trt_FP16'\n",
    "\n",
    "# Load hand sign recognizer\n",
    "hand_sign_classes = [\"0_front\", \"1_back\", \"1_front\", \"2_back\", \"2_front\", \"5_front\", \"ILU\"]\n",
    "# classification_model_path = './models/hand_sign_tf_trt_FP16'\n",
    "# classification_model_path = './models/hand_sign_saved_model'\n",
    "classification_model_path = './models/hand_sign.h5'\n",
    "\n",
    "face_detector = load_model(detection_model_path)\n",
    "# hand_sign_recongizer = load_model(classification_model_path)\n",
    "hand_sign_recongizer = tf.keras.models.load_model(classification_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                    for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_absolute(im_height, im_width, box):\n",
    "    box_abs = []\n",
    "    box_abs = [box[0] * im_height,\n",
    "               box[1] * im_width,\n",
    "               box[2] * im_height,\n",
    "               box[3] * im_width]\n",
    "    \n",
    "    return box_abs\n",
    "\n",
    "\n",
    "def convert_to_hand_dict(face_dict):\n",
    "    hand_dict = {}\n",
    "    for box_to_id in face_dict.items():\n",
    "        face_box = box_to_id[0]\n",
    "        \n",
    "        y_min = face_box[0]\n",
    "        x_min = face_box[1]\n",
    "        y_max = face_box[2]\n",
    "        x_max = face_box[3]\n",
    "\n",
    "        hand_box = [y_min, x_min-(x_max-x_min), y_max, x_min]\n",
    "\n",
    "        x_offset = (hand_box[3]-hand_box[1])*0.5\n",
    "        y_offset = (hand_box[2]-hand_box[0])*0.5\n",
    "\n",
    "        hand_box[0] -= 0.5*y_offset\n",
    "        hand_box[1] -= 2.5*x_offset\n",
    "        hand_box[2] += 1.5*y_offset\n",
    "\n",
    "        for i in range(len(hand_box)):\n",
    "            if(hand_box[i] <= 0.0):\n",
    "                hand_box[i] = 0.0\n",
    "            elif(hand_box[i] >= 1.0):\n",
    "                hand_box[i] = 1.0\n",
    "        \n",
    "        hand_dict[tuple(hand_box)] = box_to_id[1]\n",
    "\n",
    "    return hand_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Saved Model style\n",
    "# def predict_hand_sign(image):\n",
    "#     image = cv2.resize(image, (150, 150))\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "#     input_img = np.expand_dims(image, axis=0)    \n",
    "#     input_img = input_img.astype(np.float32) / 255.\n",
    "#     input_tensor = tf.constant(input_img)\n",
    "    \n",
    "#     result = hand_sign_recongizer(input_tensor)\n",
    "#     preds = result['dense_1'].numpy()\n",
    "#     return preds\n",
    "\n",
    "# Keras model(*.h5) style\n",
    "def predict_hand_sign(image):\n",
    "    image = cv2.resize(image, (150, 150))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    input_img = np.expand_dims(image, axis=0)    \n",
    "    input_img = input_img.astype(np.float32) / 255.\n",
    "    \n",
    "    preds = hand_sign_recongizer(input_img)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_box(bg_image, box, display_str, color):\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    vis_util.draw_bounding_box_on_image_array(bg_image,\n",
    "                                              ymin,\n",
    "                                              xmin,\n",
    "                                              ymax,\n",
    "                                              xmax,\n",
    "                                              color=color,\n",
    "                                              thickness=4,\n",
    "                                              display_str_list=display_str,\n",
    "                                              use_normalized_coordinates=True)\n",
    "    \n",
    "\n",
    "def detect_face_and_get_face_dict(image,\n",
    "                                  boxes,\n",
    "                                  classes,\n",
    "                                  scores,\n",
    "                                  track_ids,\n",
    "                                  use_normalized_coordinates=False,\n",
    "                                  max_boxes_to_draw=20,\n",
    "                                  min_score_thresh=.5,\n",
    "                                  line_thickness=4):\n",
    "    \n",
    "#     box_to_display_str_map = collections.defaultdict(list)\n",
    "    box_to_track_ids_map = {}\n",
    "    if not max_boxes_to_draw:\n",
    "        max_boxes_to_draw = boxes.shape[0]\n",
    "    for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "        if scores is None or scores[i] > min_score_thresh:\n",
    "            box = tuple(boxes[i].tolist())\n",
    "            box_to_track_ids_map[box] = track_ids[i]\n",
    "            \n",
    "#             display_str = 'face'\n",
    "#             display_str = '{}: {}%'.format(display_str, int(100*scores[i]))\n",
    "#             display_str = '{}: ID {}'.format(display_str, track_ids[i])\n",
    "#             box_to_display_str_map[box].append(display_str)\n",
    "            \n",
    "#             for box, display_str in box_to_display_str_map.items():\n",
    "#                 visualize_box(image, box, display_str, 'LightGreen')\n",
    "    \n",
    "    return box_to_track_ids_map\n",
    "\n",
    "\n",
    "def predict_hand_sign_and_visualize(image, hand_dict):\n",
    "    im_height, im_width, _ = image.shape\n",
    "    \n",
    "    box_to_display_str_map = collections.defaultdict(list)\n",
    "\n",
    "    for box_to_id in hand_dict.items():\n",
    "        hand_box = convert_to_absolute(im_height, im_width, list(box_to_id[0]))\n",
    "        hand_img = image[int(hand_box[0]):int(hand_box[2]),\n",
    "                         int(hand_box[1]):int(hand_box[3])]\n",
    "        \n",
    "        if hand_img.size == 0:\n",
    "            continue\n",
    "            \n",
    "        preds = predict_hand_sign(np.array(hand_img))\n",
    "        \n",
    "        if np.amax(preds[0]) > 0.85:\n",
    "            class_idx = np.argmax(preds[0])\n",
    "            class_name = hand_sign_classes[class_idx]\n",
    "            class_score = int(np.amax(preds[0])*100)\n",
    "            display_str = str(class_name)\n",
    "            display_str = '{}: {}%'.format(display_str, class_score)\n",
    "        else:\n",
    "            display_str = 'Try Again'\n",
    "        \n",
    "        box_to_display_str_map[box_to_id[0]].append(display_str)\n",
    "            \n",
    "    for box, display_str in box_to_display_str_map.items():\n",
    "        visualize_box(image, box, display_str, 'LightGrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(l):\n",
    "    if len(l) != 0:\n",
    "        return max(set(l), key = l.count) \n",
    "\n",
    "\n",
    "def get_command_box(image, video, hand_dict, duration):        \n",
    "    im_height, im_width, _ = image.shape\n",
    "    box_ids = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while(True):\n",
    "        box_to_display_str_map = collections.defaultdict(list)\n",
    "        \n",
    "        for box_to_id in hand_dict.items():\n",
    "            hand_box = convert_to_absolute(im_height, im_width, list(box_to_id[0]))\n",
    "            hand_img = image[int(hand_box[0]):int(hand_box[2]),\n",
    "                             int(hand_box[1]):int(hand_box[3])]\n",
    "        \n",
    "            if hand_img.size == 0:\n",
    "                continue\n",
    "                \n",
    "            preds = predict_hand_sign(np.array(hand_img))\n",
    "            \n",
    "            if np.amax(preds[0]) > 0.85:\n",
    "                class_idx = np.argmax(preds[0])\n",
    "                if class_idx == 5:\n",
    "                    box_ids.append(box_to_id[1])\n",
    "                    box_to_display_str_map[box_to_id[0]].append('command_box')\n",
    "            \n",
    "        for box, display_str in box_to_display_str_map.items():\n",
    "            visualize_box(image, box, display_str, 'LightGrey')\n",
    "            \n",
    "        cv2.imshow('Frame', image)\n",
    "        \n",
    "        # Press 'esc' to quit\n",
    "        if cv2.waitKey(100) == 27:\n",
    "            return -1\n",
    "        \n",
    "            \n",
    "        if(time.time()-start_time >= duration):\n",
    "            break\n",
    "            \n",
    "        ret, image = video.read()\n",
    "        \n",
    "        if ret is False:\n",
    "            print(\"Can't receive frame\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    command_box_id = most_frequent(box_ids)\n",
    "\n",
    "    if command_box_id is not None:\n",
    "        for box_to_id in hand_dict.items():\n",
    "            if box_to_id[1] == command_box_id:\n",
    "                return box_to_id[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_command(image, video, command_box, duration):\n",
    "    predictions = []\n",
    "    \n",
    "    im_height, im_width, _ = image.shape\n",
    "    command_box_abs = convert_to_absolute(im_height, im_width, list(command_box))\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while(True):\n",
    "        command_image = image[int(command_box_abs[0]):int(command_box_abs[2]),\n",
    "                              int(command_box_abs[1]):int(command_box_abs[3])]\n",
    "        \n",
    "        if command_image.size == 0:\n",
    "            continue\n",
    "        \n",
    "        preds = predict_hand_sign(np.array(command_image))\n",
    "\n",
    "        if np.amax(preds[0]) > 0.85:\n",
    "            class_idx = np.argmax(preds[0])\n",
    "            predictions.append(class_idx)\n",
    "            class_name = hand_sign_classes[class_idx]\n",
    "            display_str = str(class_name)\n",
    "        else:\n",
    "            display_str = 'Try Again'\n",
    "            \n",
    "        visualize_box(image, command_box, [display_str], 'LightGreen')\n",
    "        cv2.imshow('Frame', image)\n",
    "        \n",
    "        # Press 'esc' to quit\n",
    "        if cv2.waitKey(100) == 27:\n",
    "            return -1\n",
    "        \n",
    "        if(time.time()-start_time >= duration):\n",
    "            break\n",
    "            \n",
    "        ret, image = video.read()\n",
    "        if ret is False:\n",
    "            print(\"Can't receive frame\")\n",
    "            break\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        return None\n",
    "\n",
    "    return most_frequent(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current FPS :  30.0\n"
     ]
    }
   ],
   "source": [
    "# Frame's Width, Height\n",
    "FRAME_WIDTH = 640\n",
    "FRAME_HEIGHT = 480\n",
    "\n",
    "# Initialize webcam feed\n",
    "video = cv2.VideoCapture(0)\n",
    "# video = cv2.VideoCapture('/home/young/Desktop/test/5.mp4')\n",
    "if not video.isOpened():\n",
    "    print(\"Cannot open video\")\n",
    "    exit()\n",
    "\n",
    "video.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "video.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)   \n",
    "print('Current FPS : ', video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "max_detection = 10\n",
    "person_ids = list(range(max_detection))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if ret is False:\n",
    "        print(\"Can't receive frame\")\n",
    "        break\n",
    "        \n",
    "    output_dict = run_inference_for_single_image(face_detector, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    face_dict = detect_face_and_get_face_dict(image=frame,\n",
    "                                              boxes=output_dict['detection_boxes'],\n",
    "                                              classes=output_dict['detection_classes'],\n",
    "                                              scores=output_dict['detection_scores'],\n",
    "                                              track_ids = person_ids,\n",
    "                                              use_normalized_coordinates=True,\n",
    "                                              max_boxes_to_draw = max_detection,\n",
    "                                              line_thickness=8)\n",
    "    \n",
    "    hand_dict = convert_to_hand_dict(face_dict)\n",
    "    if hand_dict is not None:\n",
    "        command_box = get_command_box(frame, video, hand_dict, 2)\n",
    "        if command_box == -1:\n",
    "            break;\n",
    "        # send signal to arduino : command box detected\n",
    "        \n",
    "        if command_box is not None:\n",
    "            command = get_command(frame, video, command_box, 2)\n",
    "            if command == -1:\n",
    "                break\n",
    "            \n",
    "#             if command is not None:\n",
    "                # send signal to arduino : command\n",
    "\n",
    "    # Press 'esc' to quit\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
